{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimental Analysis using NLTK library of Python\n",
    "\n",
    "In this tutorial we will explore Python library NLTK and how we can use this library in Sentimental Analysis. \n",
    "We will start with the basics of Nltk and after getting some idea about it, we will then move to Sentimental Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Library\n",
    "\n",
    "How to install nltk library in python\n",
    "\n",
    "For 2.x:\n",
    "pip install nltk\n",
    "\n",
    "For mac use sudo pip install nltk\n",
    "\n",
    "For 3.x:\n",
    "pip3 install nltk \n",
    "\n",
    "For mac use sudo pip3 install nltk  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interface will be opened\n",
    "click on all and then click download. It will download all packages. This may take a while... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize sentence\n",
    "Text can be split into different sentences using nltk method sentence_tokinize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi my name is Uzair.', 'I studied from IBA.', 'And live in Pakistan']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Hi my name is Uzair. I studied from IBA. And live in Pakistan\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize words\n",
    "On the other hand we can split text or we can say data can be tokenize into words using nltk method word_tokinize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'my', 'name', 'is', 'Uzair', '.', 'I', 'studied', 'from', 'IBA', '.', 'And', 'live', 'in', 'Pakistan']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Hi my name is Uzair. I studied from IBA. And live in Pakistan\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to store words or sentence you can use arrays for storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi my name is Uzair.', 'I studied from IBA.', 'And live in Pakistan']\n",
      "['Hi', 'my', 'name', 'is', 'Uzair', '.', 'I', 'studied', 'from', 'IBA', '.', 'And', 'live', 'in', 'Pakistan']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "words = word_tokenize(text)\n",
    "print(sentences)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "A Text may contain  words like ‘am’, ‘who’, ‘where’. We can remove stopwords from the text. There is no universal list of stop words in nlp, however the nltk library provides a list of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'a',\n",
       " u'about',\n",
       " u'above',\n",
       " u'after',\n",
       " u'again',\n",
       " u'against',\n",
       " u'ain',\n",
       " u'all',\n",
       " u'am',\n",
       " u'an',\n",
       " u'and',\n",
       " u'any',\n",
       " u'are',\n",
       " u'aren',\n",
       " u\"aren't\",\n",
       " u'as',\n",
       " u'at',\n",
       " u'be',\n",
       " u'because',\n",
       " u'been',\n",
       " u'before',\n",
       " u'being',\n",
       " u'below',\n",
       " u'between',\n",
       " u'both',\n",
       " u'but',\n",
       " u'by',\n",
       " u'can',\n",
       " u'couldn',\n",
       " u\"couldn't\",\n",
       " u'd',\n",
       " u'did',\n",
       " u'didn',\n",
       " u\"didn't\",\n",
       " u'do',\n",
       " u'does',\n",
       " u'doesn',\n",
       " u\"doesn't\",\n",
       " u'doing',\n",
       " u'don',\n",
       " u\"don't\",\n",
       " u'down',\n",
       " u'during',\n",
       " u'each',\n",
       " u'few',\n",
       " u'for',\n",
       " u'from',\n",
       " u'further',\n",
       " u'had',\n",
       " u'hadn',\n",
       " u\"hadn't\",\n",
       " u'has',\n",
       " u'hasn',\n",
       " u\"hasn't\",\n",
       " u'have',\n",
       " u'haven',\n",
       " u\"haven't\",\n",
       " u'having',\n",
       " u'he',\n",
       " u'her',\n",
       " u'here',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'him',\n",
       " u'himself',\n",
       " u'his',\n",
       " u'how',\n",
       " u'i',\n",
       " u'if',\n",
       " u'in',\n",
       " u'into',\n",
       " u'is',\n",
       " u'isn',\n",
       " u\"isn't\",\n",
       " u'it',\n",
       " u\"it's\",\n",
       " u'its',\n",
       " u'itself',\n",
       " u'just',\n",
       " u'll',\n",
       " u'm',\n",
       " u'ma',\n",
       " u'me',\n",
       " u'mightn',\n",
       " u\"mightn't\",\n",
       " u'more',\n",
       " u'most',\n",
       " u'mustn',\n",
       " u\"mustn't\",\n",
       " u'my',\n",
       " u'myself',\n",
       " u'needn',\n",
       " u\"needn't\",\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'now',\n",
       " u'o',\n",
       " u'of',\n",
       " u'off',\n",
       " u'on',\n",
       " u'once',\n",
       " u'only',\n",
       " u'or',\n",
       " u'other',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'out',\n",
       " u'over',\n",
       " u'own',\n",
       " u're',\n",
       " u's',\n",
       " u'same',\n",
       " u'shan',\n",
       " u\"shan't\",\n",
       " u'she',\n",
       " u\"she's\",\n",
       " u'should',\n",
       " u\"should've\",\n",
       " u'shouldn',\n",
       " u\"shouldn't\",\n",
       " u'so',\n",
       " u'some',\n",
       " u'such',\n",
       " u't',\n",
       " u'than',\n",
       " u'that',\n",
       " u\"that'll\",\n",
       " u'the',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'them',\n",
       " u'themselves',\n",
       " u'then',\n",
       " u'there',\n",
       " u'these',\n",
       " u'they',\n",
       " u'this',\n",
       " u'those',\n",
       " u'through',\n",
       " u'to',\n",
       " u'too',\n",
       " u'under',\n",
       " u'until',\n",
       " u'up',\n",
       " u've',\n",
       " u'very',\n",
       " u'was',\n",
       " u'wasn',\n",
       " u\"wasn't\",\n",
       " u'we',\n",
       " u'were',\n",
       " u'weren',\n",
       " u\"weren't\",\n",
       " u'what',\n",
       " u'when',\n",
       " u'where',\n",
       " u'which',\n",
       " u'while',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'why',\n",
       " u'will',\n",
       " u'with',\n",
       " u'won',\n",
       " u\"won't\",\n",
       " u'wouldn',\n",
       " u\"wouldn't\",\n",
       " u'y',\n",
       " u'you',\n",
       " u\"you'd\",\n",
       " u\"you'll\",\n",
       " u\"you're\",\n",
       " u\"you've\",\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "# set function is an unordered collection with no duplicate elements \n",
    "# For Example: x = [1, 1, 2, 2, 2, 2, 2, 3, 3]\n",
    "# set(x) # use set\n",
    "# set([1, 2, 3]) # print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In below example we will remove stopwords from sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stopwords', 'code', 'which', 'contain', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "After removing stopwords\n",
      "['Stopwords', 'code', 'contain', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sample = \"Stopwords code which contain a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "stop_words_array = set(stopwords.words('english'))\n",
    "\n",
    "words = word_tokenize(sample)\n",
    "filtered_sentence =  []\n",
    "for w in words:\n",
    "    if w not in stop_words_array:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(words)\n",
    "print('After removing stopwords')\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.710579"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import movie_reviews\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "useless_words = stopwords.words('english') + list(string.punctuation)\n",
    "#all_words = movie_reviews.words()\n",
    "filtered_words = [word for word in movie_reviews.words() if not word in useless_words]\n",
    "len(filtered_words)/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print out the most common words from filtered words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 9517),\n",
       " ('one', 5852),\n",
       " ('movie', 5771),\n",
       " ('like', 3690),\n",
       " ('even', 2565),\n",
       " ('good', 2411),\n",
       " ('time', 2411),\n",
       " ('story', 2169),\n",
       " ('would', 2109),\n",
       " ('much', 2049)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counter = Counter(filtered_words)\n",
    "most_commond_words = word_counter.most_common()[:10]\n",
    "most_commond_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Example\n",
    "\n",
    "We will use Classification approach for classiy out result and this can be done using Traing and Testing.\n",
    "\n",
    "The training step needs to have data. The classifier will use this training data to make predictions.\n",
    "\n",
    "We start by defining 2 classes: positive, negative.\n",
    "We will define the vocabulary of these classes:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\n",
    "negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_features(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_features = [(word_features(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_features(neg), 'neg') for neg in negative_vocab]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will add positive_features and negative_features for training set and after that we will clasify using NaiveBayesClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NaiveBayesClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-cf410260c534>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtraining_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnegative_features\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpositive_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'NaiveBayesClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "training_set = negative_features + positive_features\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 0.6\n",
      "Negative: 0.4\n"
     ]
    }
   ],
   "source": [
    "# Below is the example which classifies sentences according to the training set.\n",
    "\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\n",
    "negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\n",
    "neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    " \n",
    "training_set = negative_features + positive_features \n",
    " \n",
    "classifier = NaiveBayesClassifier.train(training_set) \n",
    " \n",
    "# Predict\n",
    "neg = 0\n",
    "pos = 0\n",
    "sentence = \"Awesome movie, but bad music\"\n",
    "sentence = sentence.lower()\n",
    "words = sentence.split(' ')\n",
    "for word in words:\n",
    "    classResult = classifier.classify( word_feats(word))\n",
    "    if classResult == 'neg':\n",
    "        neg = neg + 1\n",
    "    if classResult == 'pos':\n",
    "        pos = pos + 1\n",
    " \n",
    "print('Positive: ' + str(float(pos)/len(words)))\n",
    "print('Negative: ' + str(float(neg)/len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, through this example you will get some idea of how to use training data and how well we predict about the new data. The better your training data is, the more accurate your result will be. In the example our training data is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Movie Corpus\n",
    "\n",
    "We will use bag-of-words in below example, as the word says bag-of-words is simply a collection of words disregarding order or grammer but all words will be unique.\n",
    "\n",
    "Classification using machine learning is a technique used for sentiment models. We will build a sentiment classifer using the movie review corpus. \n",
    "Classification is a technique which requires labels from data. This is where we will take advantage of bag-of-words and a curated negative and positive reviews we downloaded.\n",
    "We will implement bag-of-words function to create a positive or negative label for each review bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "def build_bag_of_words_features(words):\n",
    "    return {\n",
    "        word:1 for word in words if not word in useless_words}\n",
    "\n",
    "positive_reviews = movie_reviews.fileids('pos')\n",
    "negative_reviews = movie_reviews.fileids('neg')\n",
    "\n",
    "negative_features = [ (build_bag_of_words_features(movie_reviews.words(fileids = [f])), 'neg')\n",
    "                   for f in negative_reviews]\n",
    "positive_features = [ (build_bag_of_words_features(movie_reviews.words(fileids = [f])), 'pos')\n",
    "                   for f in positive_reviews]\n",
    "\n",
    "print(len(negative_features))\n",
    "print(len(positive_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Naive Bayes Classifier for this task. It is a very simple classifier with a probabilistic approach to classification. What this means is that the relationships between the input features and the class labels is expressed as probabilities. So, given the input features, for example, the probability for each class is estimated. The class with the highest probability then determines the label for the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest supervised machine learning classifiers is the Naive Bayes Classifier, we will train on 80% of the data what words are generally associated with positive or with negative reviews\n",
    "\n",
    "Remember, we had 1,000 records in both of positive and negative features. We can use 80% of the data for classification in Naive Bayes. When we provide the first 800 rows in each feature, it's 80%. So we'll store that number, 800, in a variable called split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the\n",
    "Naive Bayes Classifier to build a classifier\n",
    "that we will call Sentiment Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_classifier = NaiveBayesClassifier.train(positive_features[:split] + negative_features[:split] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will classify with the first 800 positive features\n",
    "and the first 800 negative features.\n",
    "Remember they had labels pos and neg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.980625"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.util.accuracy(sentiment_classifier, positive_features[:split] + negative_features[:split] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it's about 98% accuracy, so it's good.\n",
    "But how will the model behave on the 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7175"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.classify.util.accuracy(sentiment_classifier, positive_features[split:] + negative_features[split:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the accuracy of it, if we calculate it is around 71%.\n",
    "The estimated accuracy for a human is about 80%.\n",
    "So an accuracy of around 70% is a pretty good accuracy\n",
    "for such a simple model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we had a large vocabulary and\n",
    "the Sentiment Classifier used all the words,\n",
    "but which of those words gave us this highish accuracy?\n",
    "The Sentiment Classifier, the one model we built,\n",
    "has a function, it says, show most informative features.\n",
    "We can actually run that and see what words\n",
    "or what features in those reviews were most informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             outstanding = 1                 pos : neg    =     13.9 : 1.0\n",
      "               insulting = 1                 neg : pos    =     13.7 : 1.0\n",
      "              vulnerable = 1                 pos : neg    =     13.0 : 1.0\n",
      "               ludicrous = 1                 neg : pos    =     12.6 : 1.0\n",
      "             uninvolving = 1                 neg : pos    =     12.3 : 1.0\n",
      "              astounding = 1                 pos : neg    =     11.7 : 1.0\n",
      "                  avoids = 1                 pos : neg    =     11.7 : 1.0\n",
      "             fascination = 1                 pos : neg    =     11.0 : 1.0\n",
      "               affecting = 1                 pos : neg    =     10.3 : 1.0\n",
      "                  symbol = 1                 pos : neg    =     10.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "sentiment_classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this tutorial we start with basics of nltk library and goes to how we can use it in Sentimental Analysis. There are many training datasets available online. Like [University dataset](https://github.com/nltk/nltk/wiki/Sentiment-Analysis).\n",
    "A good dataset will increase the accuracy of your classifier. More the data better the result will be.\n",
    "\n",
    "Thanks for reading I hope that you found this tutorial helpful. Feel free to reach me on twitter [@UzairAdamjee](https://twitter.com/UzairAdamjee)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
